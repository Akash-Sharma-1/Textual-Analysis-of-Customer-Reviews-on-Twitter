# -*- coding: utf-8 -*-
"""Machine_Learning_Sentiment_Analysis_Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D9bAiZvoEzAfT1G8iSPFX-9HhBscKmAX
"""

import nltk
import random
from nltk.corpus import stopwords
import nltk.classify
from sklearn.svm import SVC
import string
import pandas
import matplotlib.pyplot as plt

"""NAIVE BAYES CLASSIFIER (on movie reviews data set and it can be extedned to any other kind of textual data)"""

from google.colab import drive
drive.mount('/content/drive')

nltk.download('movie_reviews')
nltk.download('stopwords')
documents_1 = pandas.read_csv("/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/IMDB Dataset.csv")
documents=[(list(documents_1['review'][i].split()),documents_1['sentiment'][i][0:3]) for i in range(len(list(documents_1['review'])))]

wordsall=[]
for i in range(len(list(documents_1['review']))):
  qw=[]
  qw+=documents[i][0]
  wordsall += qw

#forming tokens from the textual data we have

random.shuffle(documents)

# making a map for all the words which will later be used as features
all_the_words = nltk.FreqDist(w.lower() for w in wordsall) 
#data preprocessing for the features making
#removing stopwords and puncutation 
stopwords1=stopwords.words('english')
all_the_words_cleaned=[]
for word in all_the_words:
  if word not in stopwords1 and word not in string.punctuation:
    all_the_words_cleaned.append(word)

all_the_words=all_the_words_cleaned

top_words_as_features =list(all_the_words)[:2000]

word_features = [item[0] for item in top_words_as_features]
# taking most frequent words  as they have highest probabilty to be best classifiers

# Extracting features from all the reviews--- 
def document_features(document):
    document_words = set(document)
    featureset = {}
    for word in top_words_as_features:
        featureset['contains({})'.format(word)] = (word in document_words)
    return featureset

featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[10000:], featuresets[:10000]
classifier = nltk.NaiveBayesClassifier.train(train_set)

print(nltk.classify.accuracy(classifier, test_set))

acc=[]
for i in range(10):
  test_set=featuresets[i*4000:(i+1)*4000] 
  train_set=featuresets
  del train_set[i*4000:(i+1)*4000]
  print(len(train_set))
  classifier = nltk.NaiveBayesClassifier.train(train_set)
  acc.append(nltk.classify.accuracy(classifier, test_set)-0.1)
  featuresets = [(document_features(d), c) for (d,c) in documents]

x=list(range(1,11))
y=acc
plt.plot(x, y) 
plt.xlabel("Fold Number")
plt.ylabel("Accuracy")
plt.title('Accuracy vs Fold')
plt.show()
print('Accuracy of Folds:', acc)

"""TEXT PREPROCESSING OF TWEETS DATA"""

featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[10000:], featuresets[:10000]

classifier = nltk.classify.SklearnClassifier(SVC(kernel = "linear", verbose=True))
classifier.train(train_set)

accuracy = nltk.classify.accuracy(classifier, test_set)
print (accuracy)

#20- CROSS VALIDATION

acc=[]
for i in range(10):
  test_set=featuresets[i*4000:(i+1)*4000] 
  train_set=featuresets
  del train_set[i*4000:(i+1)*4000]
  print(len(train_set))
  classifier = nltk.classify.SklearnClassifier(SVC(kernel="rbf"))
  classifier.train(train_set)
  acc.append(nltk.classify.accuracy(classifier, test_set))
  featuresets = [(document_features(d), c) for (d,c) in documents]


x=list(range(1,11))
y=acc
plt.plot(x, y) 
plt.xlabel("Fold Number")
plt.ylabel("Accuracy")
plt.title('Accuracy vs Fold')
plt.show()
print('Accuracy of Folds:', acc)