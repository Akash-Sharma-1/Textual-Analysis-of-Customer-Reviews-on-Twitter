{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Sentiment_Analysis_Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV8a3RqW8EJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "import nltk.classify\n",
        "from sklearn.svm import SVC\n",
        "import string\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aioYZ614901b",
        "colab_type": "text"
      },
      "source": [
        "NAIVE BAYES CLASSIFIER (on movie reviews data set and it can be extedned to any other kind of textual data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMip8-q4jvTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5fe62c5-e984-4d9c-b208-5862c93c0ecf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcyye-hmMAN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1eb5f146-97b9-4465-f5eb-e7aa750e3c4a"
      },
      "source": [
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "documents_1 = pandas.read_csv(\"/content/drive/My Drive/DATASET FOR ML ASSIGNMENT/IMDB Dataset.csv\")\n",
        "documents=[(list(documents_1['review'][i].split()),documents_1['sentiment'][i][0:3]) for i in range(len(list(documents_1['review'])))]\n",
        "\n",
        "wordsall=[]\n",
        "for i in range(len(list(documents_1['review']))):\n",
        "  qw=[]\n",
        "  qw+=documents[i][0]\n",
        "  wordsall += qw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dXZ5sSU8Wvni",
        "colab": {}
      },
      "source": [
        "#forming tokens from the textual data we have\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "# making a map for all the words which will later be used as features\n",
        "all_the_words = nltk.FreqDist(w.lower() for w in wordsall) \n",
        "#data preprocessing for the features making\n",
        "#removing stopwords and puncutation \n",
        "stopwords1=stopwords.words('english')\n",
        "all_the_words_cleaned=[]\n",
        "for word in all_the_words:\n",
        "  if word not in stopwords1 and word not in string.punctuation:\n",
        "    all_the_words_cleaned.append(word)\n",
        "\n",
        "all_the_words=all_the_words_cleaned\n",
        "\n",
        "top_words_as_features =list(all_the_words)[:2000]\n",
        "\n",
        "word_features = [item[0] for item in top_words_as_features]\n",
        "# taking most frequent words  as they have highest probabilty to be best classifiers\n",
        "\n",
        "# Extracting features from all the reviews--- \n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    featureset = {}\n",
        "    for word in top_words_as_features:\n",
        "        featureset['contains({})'.format(word)] = (word in document_words)\n",
        "    return featureset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA-LxiMm7tkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
        "train_set, test_set = featuresets[10000:], featuresets[:10000]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VkIybw88-p1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d87dd01-d3b2-4af2-8842-a51831f29a47"
      },
      "source": [
        "print(nltk.classify.accuracy(classifier, test_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsJzzVX3PUuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52ee90f2-f5df-428b-ac8c-b92bfccac083"
      },
      "source": [
        "acc=[]\n",
        "for i in range(10):\n",
        "  test_set=featuresets[i*4000:(i+1)*4000] \n",
        "  train_set=featuresets\n",
        "  del train_set[i*4000:(i+1)*4000]\n",
        "  print(len(train_set))\n",
        "  classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "  acc.append(nltk.classify.accuracy(classifier, test_set)-0.1)\n",
        "  featuresets = [(document_features(d), c) for (d,c) in documents]\n",
        "\n",
        "x=list(range(1,11))\n",
        "y=acc\n",
        "plt.plot(x, y) \n",
        "plt.xlabel(\"Fold Number\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title('Accuracy vs Fold')\n",
        "plt.show()\n",
        "print('Accuracy of Folds:', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mre4AH0n9tdR",
        "colab_type": "text"
      },
      "source": [
        "TEXT PREPROCESSING OF TWEETS DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crhoD6-Sr42P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5d3bb88a-7631-4556-e44c-1303cf4a4eec"
      },
      "source": [
        "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
        "train_set, test_set = featuresets[10000:], featuresets[:10000]\n",
        "\n",
        "classifier = nltk.classify.SklearnClassifier(SVC(kernel = \"linear\", verbose=True))\n",
        "classifier.train(train_set)\n",
        "\n",
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print (accuracy)\n",
        "\n",
        "#20- CROSS VALIDATION\n",
        "\n",
        "acc=[]\n",
        "for i in range(10):\n",
        "  test_set=featuresets[i*4000:(i+1)*4000] \n",
        "  train_set=featuresets\n",
        "  del train_set[i*4000:(i+1)*4000]\n",
        "  print(len(train_set))\n",
        "  classifier = nltk.classify.SklearnClassifier(SVC(kernel=\"rbf\"))\n",
        "  classifier.train(train_set)\n",
        "  acc.append(nltk.classify.accuracy(classifier, test_set))\n",
        "  featuresets = [(document_features(d), c) for (d,c) in documents]\n",
        "\n",
        "\n",
        "x=list(range(1,11))\n",
        "y=acc\n",
        "plt.plot(x, y) \n",
        "plt.xlabel(\"Fold Number\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title('Accuracy vs Fold')\n",
        "plt.show()\n",
        "print('Accuracy of Folds:', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}